{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eaf38eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from torch import nn, optim\n",
    "\n",
    "from nnbma.networks import DenselyConnected, PolynomialNetwork, FullyConnected\n",
    "from nnbma.dataset import RegressionDataset, MaskDataset\n",
    "from nnbma.learning import learning_procedure, LearningParameters, MaskedMSELoss, LinearBatchScheduler, BatchScheduler\n",
    "\n",
    "from nnbma.layers import PolynomialExpansion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4393265d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def error_factor (y_hat: np.ndarray, y: np.ndarray) :\n",
    "     ef_vector=np.exp(np.log(10)*np.abs(y_hat - y))\n",
    "     return (np.percentile(ef_vector,99),np.mean(ef_vector))\n",
    "\n",
    "def metric(y_hat: np.ndarray, y: np.ndarray):\n",
    "    return np.mean((y_hat - y) ** 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e7eedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_root = \"../data/models\"\n",
    "data_dir = \"../data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3769ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset = pd.read_csv(\"../data/dataset_train_test.csv\", index_col=0)\n",
    "\n",
    "idx = np.arange(len(df_dataset))\n",
    "train_frac = 0.7\n",
    "idx_max_train = int(0.7 * len(df_dataset))\n",
    "\n",
    "df_train = df_dataset.iloc[:idx_max_train, :] * 1\n",
    "df_test = df_dataset.iloc[idx_max_train:, :] * 1\n",
    "\n",
    "X_train = np.log10(df_train.iloc[:, :3].values)\n",
    "Y_train = np.log10(df_train.iloc[:, 3:].values)\n",
    "X_test = np.log10(df_test.iloc[:, :3].values)\n",
    "Y_test = np.log10(df_test.iloc[:, 3:].values)\n",
    "\n",
    "X_labels = list(df_train.columns)[:3]\n",
    "Y_labels = list(df_train.columns)[3:]\n",
    "\n",
    "X_labels, Y_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991eda71",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8447875",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704100ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%  Normalisation des données entre 0 et 1\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "train_dataset = RegressionDataset(X_train, Y_train)\n",
    "test_dataset = RegressionDataset(X_test, Y_test)\n",
    "print(f\"Number of training entries: {X_train.shape[0]:,}\") # 154\n",
    "print(f\"Number of testing entries: {X_test.shape[0]:,}\") #20 #49\n",
    "\n",
    "#%%  Procedure for training\n",
    "\n",
    "n_layers = 4\n",
    "growing_factor = 0.5\n",
    "\n",
    "activation = nn.GELU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0862f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------Expansion Polynomiale\n",
    "order = 3  #pas + sinon surentraine\n",
    "n_features = 3\n",
    "\n",
    "n_poly = PolynomialExpansion.expanded_features(order,n_features)\n",
    "#Création d'une premiere couche qui fait passer de 2 à n params par expansion polynomiale\n",
    "subnet = DenselyConnected(n_poly, L, n_layers, growing_factor, activation, outputs_names=Y_labels)\n",
    "\n",
    "#Création du réseau en prenant le subnet en premiere ligne\n",
    "net = PolynomialNetwork(\n",
    "    n_features,\n",
    "    order,\n",
    "    subnet,\n",
    "    inputs_names=X_labels,\n",
    "    outputs_names=Y_labels\n",
    ")\n",
    "\n",
    "net.poly.update_standardization(x=train_dataset.x, reset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90b24fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff87f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.count_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5091fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "batch_size = len(df_train)\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "learning_rate = 1e-2\n",
    "optimizer = optim.AdamW(net.parameters(), learning_rate)\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, patience=20, factor=0.9\n",
    ")\n",
    "\n",
    "learning_params = LearningParameters(\n",
    "    loss, epochs, batch_size, optimizer, scheduler\n",
    ")\n",
    "\n",
    "results = learning_procedure(\n",
    "    net,\n",
    "    (train_dataset, test_dataset),\n",
    "    learning_params,\n",
    "    val_frac=None,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "#%% Partie Résultats\n",
    "print(f\"Error Factor over training set: {error_factor(net(X_train), Y_train)}\")\n",
    "print(f\"Error Factor over testing set: {error_factor(net(X_test), Y_test)}\")\n",
    "\n",
    "print(f\"Loss over training set: {metric(net(X_train), Y_train):.2e}\")\n",
    "print(f\"Loss over testing set: {metric(net(X_test), Y_test):.2e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f254d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(1.2 * 6.4, 0.6 * 4.8))\n",
    "\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.semilogy(results[\"train_loss\"][100:],\"--\",label=\"Train loss\")\n",
    "plt.semilogy(results[\"val_loss\"][100:], label=\"Test loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Logarithmic scale\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.semilogy(results[\"lr\"][100:],\"--\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.title(\"Logarithmic scale\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68fc32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "name=\"model\"\n",
    "net.save(name, output_root)\n",
    "\n",
    "import pickle\n",
    "with open(f\"{output_root}/{name}/scaler.pickle\", 'wb') as handle:\n",
    "    pickle.dump(scaler, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f7b748",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "beetroots-tuto-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
